{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install gensim\n",
    "!{sys.executable} -m pip install pyLDAvis\n",
    "!{sys.executable} -m pip install picky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import json\n",
    "import nltk\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import string\n",
    "\n",
    "from plotly import graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "from nltk.sentiment.util import demo_liu_hu_lexicon\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import opinion_lexicon, stopwords, sentiwordnet as swn\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path, name):\n",
    "  import os\n",
    "\n",
    "  if not os.path.exists(f'{path}/{name}'):\n",
    "    try:\n",
    "      os.makedirs(f'{path}/{name}')\n",
    "      return f'{path}/{name}'\n",
    "    except OSError as e:\n",
    "      print(e)\n",
    "      exit()\n",
    "\n",
    "  return f'{path}/{name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir('./data', 'open-ended')\n",
    "survey_df = pd.read_csv('./data/MSD Survey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_questions = {\n",
    "  0: \"Since how many semesters have you been studying?\",\n",
    "  1: \"How many hours per week do you spend on a homework?\",\n",
    "  2: \"Are you satisfied with the topics taught in the lecture?\",\n",
    "  3: \"Do you intend to use the topics you learned outside of the course? [Descriptive Statistics]\",\n",
    "  4: \"Do you intend to use the topics you learned outside of the course? [Data Mining]\",\n",
    "  5: \"Do you intend to use the topics you learned outside of the course? [Explorative Analysis]\",\n",
    "  6: \"Do you intend to use the topics you learned outside of the course? [Prediction model]\",\n",
    "  7: \"Do you intend to use the topics you learned outside of the course? [Natural Language Processing]\",\n",
    "  8: \"Do you intend to use the topics you learned outside of the course? [Python]\",\n",
    "  9: \"Do you intend to use the topics you learned outside of the course? [Jupyter Notebook]\",\n",
    "  10: \"Do you intend to use the topics you learned outside of the course? [Experimentation]\",\n",
    "  11: \"Do you intend to use the topics you learned outside of the course? [Surveys]\",\n",
    "  12: \"Rate the difficulty of the lecture\",\n",
    "  13: \"Would you attend this course again?\",\n",
    "  14: \"Would you recommend this course to others?\"\n",
    "}\n",
    "\n",
    "oe_questions = {\n",
    "  0: \"What do you study?\",\n",
    "  1: \"What is your profession?\",\n",
    "  2: \"Why did you choose the lecture MSD? \",\n",
    "  3: \"What did you miss in the lecture? \",\n",
    "  4: \"Reflect on the repository mining project (topic selection, were your expectations fulfilled, learnings, etc.)\",\n",
    "  5: \"Why do you think that the weekly exercise sheets were a good preparation for the projects or not?\",\n",
    "  6: \"What kind of extra material did you use to solve the weekly homeworks and the project?\",\n",
    "  7: \"What did you like in the course?\",\n",
    "  8: \"What did you not like in the course?\",\n",
    "  9: \"How do you agree with the  following statement: The topics taught in the lecture can be applied in various application areas\",\n",
    "  10:\"What would you suggest for the improvement of the lecture?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Ended Questions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ce_question_stats(question):\n",
    "  labels = survey_df[question].value_counts().reset_index().values[:, 0]\n",
    "  values = survey_df[question].value_counts().reset_index().values[:, 1]\n",
    "\n",
    "  iplot({\n",
    "    \"data\": [{\n",
    "        \"values\": values,\n",
    "        \"labels\": labels,\n",
    "        \"type\": \"pie\"\n",
    "        }],\n",
    "      \"layout\": { \"title\": question}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for q in ce_questions:\n",
    "  plot_ce_question_stats(ce_questions[q])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Remove stopwords and punctuation, lemmatize words\n",
    "def clean(text):\n",
    "  text = stemmer.stem(text)\n",
    "  no_stop = ' '.join([i for i in text.lower().split() if i not in stop])\n",
    "  no_punc = ''.join(ch for ch in no_stop if ch not in exclude)\n",
    "  cleaned = ' '.join(lemma.lemmatize(word) for word in no_punc.split())\n",
    "\n",
    "  return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lda_model(question_id, answers, num_topics=3, passes=50):\n",
    "  path = f'./data/open-ended'\n",
    "  path = create_dir(path, question_id)\n",
    "\n",
    "  dictionary = corpora.Dictionary(answers)\n",
    "  doc_term_matrix = [dictionary.doc2bow(answ) for answ in answers]\n",
    "  \n",
    "  pickle.dump(doc_term_matrix, open(f'{path}/corpus.pkl', 'wb'))\n",
    "  dictionary.save(f'{path}/dictionary.gensim')\n",
    "  \n",
    "  lda_model = LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "  lda_model.save(f'{path}/model.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_lda_model(path, num_terms=10):\n",
    "  dictionary = gensim.corpora.Dictionary.load(f'{path}/dictionary.gensim')\n",
    "  corpus = pickle.load(open(f'{path}/corpus.pkl', 'rb'))\n",
    "  \n",
    "  lda = LdaModel.load(f'{path}/model.gensim')\n",
    "  lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, R=num_terms)\n",
    "  \n",
    "  return lda_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in oe_questions:\n",
    "  clean_text = []\n",
    "  [clean_text.append(clean(answ).split()) for answ in survey_df[oe_questions[i]] if answ is not np.nan]\n",
    "  compute_lda_model(i, clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you study?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(display_lda_model('./data/open-ended/0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_obj = lambda pos, neu, neg: {\n",
    "  'positive': pos,\n",
    "  'neutral': neu,\n",
    "  'negative': neg\n",
    "}\n",
    "\n",
    "positive_vocab = ['interesting', 'informative', 'satisfied', 'learned', 'good', 'nice', 'great', 'awesome', 'well', 'fantastic', 'enjoy', 'agree']\n",
    "negative_vocab = ['difficult', 'bad', 'terrible', 'useless', 'hate', 'long', 'boring']\n",
    "\n",
    "positive_vocab = [stemmer.stem(i) for i in positive_vocab]\n",
    "negative_vocab = [stemmer.stem(i) for i in negative_vocab]\n",
    "\n",
    "positive_words = set(opinion_lexicon.positive()).union(positive_vocab)\n",
    "negative_words = set(opinion_lexicon.negative()).union(negative_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_of(text):\n",
    "  if text is np.nan:\n",
    "    return sentiment_obj(0, 0 ,0)\n",
    "  \n",
    "  if not text.endswith('.'):\n",
    "    text += '.'\n",
    "\n",
    "  tokenized = word_tokenize(clean(text))\n",
    "  total = len(tokenized)\n",
    "\n",
    "  pos = 0\n",
    "  neg = 0\n",
    "  neu = 0\n",
    "  \n",
    "  for t in tokenized:\n",
    "    if t in positive_words:\n",
    "      pos += 1\n",
    "    elif t in negative_words:\n",
    "      neg += 1\n",
    "    else:\n",
    "      neu += 1\n",
    "  \n",
    "  return sentiment_obj(round(pos / total, 4),\n",
    "                       round(neu / total, 4),\n",
    "                       round(neg / total, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_of_answers(answers):\n",
    "  pos = []\n",
    "  neu = []\n",
    "  neg = []\n",
    "\n",
    "  for a in answers:\n",
    "    res = sentiment_of(a)\n",
    "    pos.append(res['positive'])\n",
    "    neu.append(res['neutral'])\n",
    "    neg.append(res['negative'])\n",
    "    \n",
    "  return sentiment_obj(round(np.mean(pos), 4),\n",
    "                       round(np.mean(neu), 4),\n",
    "                       round(np.mean(neg), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_sentiments():\n",
    "  sentiments = dict()\n",
    "  for q in oe_questions:\n",
    "    question = oe_questions[q]\n",
    "    sentiments[question] = sentiment_of_answers(survey_df[oe_questions[q]])\n",
    "  \n",
    "  with open(f'./data/sentiments.json', 'w+', encoding='utf-8') as f:\n",
    "    json.dump(sentiments, f, indent=2)\n",
    "  \n",
    "  return sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mean sentiments of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pprint(store_sentiments())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
