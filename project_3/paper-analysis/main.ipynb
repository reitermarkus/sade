{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!{sys.executable} -m pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('word_tokenize')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/papers.json', 'r', encoding = 'utf-8') as f:\n",
    "  papers = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/papers.json', 'r', encoding='utf-8') as f:\n",
    "  data = json.load(f)\n",
    "  \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import os\n",
    "\n",
    "def build_model(tagged_data, name):\n",
    "  model = Doc2Vec(\n",
    "    vector_size = 20,\n",
    "    alpha = 0.025, \n",
    "    min_alpha = 0.00025,\n",
    "    min_count = 1,\n",
    "    dm = 1,\n",
    "  )\n",
    "\n",
    "  model.build_vocab(tagged_data)\n",
    "\n",
    "  max_epochs = 100\n",
    "\n",
    "  for epoch in range(max_epochs):\n",
    "    model.train(\n",
    "      tagged_data,\n",
    "      total_examples = model.corpus_count,\n",
    "      epochs = model.epochs,\n",
    "    )\n",
    "    \n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    \n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "  model.save(f'data/{name}.model')\n",
    "  \n",
    "  return model\n",
    "\n",
    "tagged_titles = [\n",
    "  TaggedDocument(words = clean(paper['title']), tags=[paper['title']]) \n",
    "  for paper in papers\n",
    "]\n",
    "\n",
    "tagged_titles_and_abstracts = [\n",
    "  TaggedDocument(words = clean(paper['title']) + clean(paper['abstract']), tags=[paper['title']]) \n",
    "  for paper in papers\n",
    "]\n",
    "  \n",
    "title_model = build_model(tagged_titles, 'title')\n",
    "title_and_abstract_model = build_model(tagged_titles_and_abstracts, 'title_and_abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "def generate_prediction(model, n_clusters = 4):\n",
    "  return KMeans(init = 'k-means++', n_clusters = n_clusters).fit(model.docvecs.vectors_docs)\n",
    "\n",
    "def cluster_table(model):\n",
    "  kmeans = generate_prediction(model)\n",
    "\n",
    "  trace = go.Table(\n",
    "    header = dict(values = ['Title', 'Cluster ID']),\n",
    "    cells = dict(values = [model.docvecs.offset2doctag,kmeans.labels_]),\n",
    "  )\n",
    "\n",
    "  data = [trace]\n",
    "\n",
    "  figure = go.Figure(data = data)\n",
    "\n",
    "  return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iplot(cluster_table(title_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iplot(cluster_table(title_and_abstract_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def find_most_similar_vectors(tagged_data, similar_to, model_name, create_model=False, topn=5):\n",
    "  # similar_to: tag (int) or text\n",
    "\n",
    "  if create_model:\n",
    "    create_and_train(tagged_abstracts, model_name)\n",
    "  \n",
    "  model = Doc2Vec.load(f'data/{model_name}')\n",
    "\n",
    "  if isinstance(similar_to, int):\n",
    "    pass\n",
    "  elif isinstance(similar_to, str):\n",
    "    similar_to = [model.infer_vector(word_tokenize(similar_to))]\n",
    "  \n",
    "  similar_vectors = model.docvecs.most_similar(similar_to, topn = topn)\n",
    "    \n",
    "  result = [(tag, value) for tag, value in similar_vectors]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Example Most Similar Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check = 'A Domain Specific Language based on Monads for Distributed Transactional Memory in Java'\n",
    "cmp_string = find_most_similar_vectors(tagged_titles, to_check, 'title.model', topn = 10, create_model = False)\n",
    "cmp_tag = find_most_similar_vectors(tagged_titles, 1, 'title.model', topn = 10, create_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_dir('./data/topic_modeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def papers_topic_modeling(field_name):\n",
    "  clean_text = [clean(d[field_name]) for d in data]\n",
    "\n",
    "  compute_lda_model(f'./data/topic_modeling/{field_name}', clean_text)\n",
    "  \n",
    "def display_topic_modeling(field_name):\n",
    "  return display_lda_model(f'./data/topic_modeling/{field_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "papers_topic_modeling('title')\n",
    "pyLDAvis.display(display_topic_modeling('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "papers_topic_modeling('abstract')\n",
    "pyLDAvis.display(display_topic_modeling('abstract'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
